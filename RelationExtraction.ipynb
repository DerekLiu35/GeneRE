{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9LzIqcwiY0Qnu0dF4n2ie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DerekLiu35/GeneRE/blob/main/RelationExtraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/michiyasunaga/LinkBERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMgrfiVPeobd",
        "outputId": "82f939dd-cc36-4e36-a16d-74cebecd996e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LinkBERT'...\n",
            "remote: Enumerating objects: 42, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 42 (delta 11), reused 42 (delta 11), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (42/42), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create -n linkbert python=3.8\n",
        "!source activate linkbert\n",
        "!pip install torch==1.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!pip install transformers==4.9.1 datasets==1.11.0 fairscale==0.4.0 wandb sklearn seqeval"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHlT5ZzUaVV_",
        "outputId": "4a2c473d-1b69-43ff-e81c-53727fef7303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: conda: command not found\n",
            "/bin/bash: activate: No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
            "Collecting torch==1.10.1+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.1%2Bcu113-cp38-cp38-linux_x86_64.whl (1821.4 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m169.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1821442048 bytes == 0x3228000 @  0x7f702feb71e7 0x4d30a0 0x4d312c 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m208.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 2276802560 bytes == 0x6fb38000 @  0x7f702feb8615 0x5d6f4c 0x51edd1 0x51ef5b 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x5d8868 0x4997a2 0x55cd91 0x5d8941 0x49abe4 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m208.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mtcmalloc: large alloc 1821442048 bytes == 0x3228000 @  0x7f702feb71e7 0x4d30a0 0x5dede2 0x6758aa 0x4f750a 0x4997a2 0x55cd91 0x5d8941 0x4997a2 0x55cd91 0x5d8941 0x4fe318 0x5da092 0x62042c 0x5d8d8c 0x561f80 0x4fd2db 0x4997c7 0x4fd8b5 0x4997c7 0x4fd8b5 0x49abe4 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x4f5fe9 0x55e146 0x5d8868 0x5da092 0x587116\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m959.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.1+cu113) (4.4.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.0+cu116\n",
            "    Uninstalling torch-1.13.0+cu116:\n",
            "      Successfully uninstalled torch-1.13.0+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.14.0+cu116 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\n",
            "torchtext 0.14.0 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\n",
            "torchaudio 0.13.0+cu116 requires torch==1.13.0, but you have torch 1.10.1+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.10.1+cu113\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.9.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.11.0\n",
            "  Downloading datasets-1.11.0-py3-none-any.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.9/264.9 KB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fairscale==0.4.0\n",
            "  Downloading fairscale-0.4.0.tar.gz (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.9-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (3.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (2.25.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (4.64.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (2022.6.2)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (6.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0) (0.3.6)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0) (2022.11.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==1.11.0) (1.3.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from fairscale==0.4.0) (1.10.1+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.1) (4.4.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.13.0-py2.py3-none-any.whl (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 KB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.9.1) (3.0.9)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (2022.12.7)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.11.0) (2022.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.11.0) (2.8.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: fairscale, sklearn, seqeval, pathtools, sacremoses\n",
            "  Building wheel for fairscale (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairscale: filename=fairscale-0.4.0-py3-none-any.whl size=239949 sha256=6a834050d8ddbff6859b0a2cc89a12b9b69686329e51e98c43869026cbc4ce1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/a8/59/b9f425839b6970156e701b1ea4661f9f61cbc1a0f57fa2f214\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=121748d20d4c9a3e585f7c4412097d095fa1101efc65d046ab3237a56b9e3c76\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=cbf89d9748159350d929eb55e03584d3270d2c5529ecd20507f509a7009b097f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f1fca62fc3ae3aa2d52e2e5838c895978ddae6cecd3f30cd46f7ce653a69d7f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=5c1a9aef9373125b136b4bb2e106b240290b793927c0b00a49d4619ef9b7ff94\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built fairscale sklearn seqeval pathtools sacremoses\n",
            "Installing collected packages: tokenizers, sklearn, pathtools, xxhash, urllib3, smmap, setproctitle, sacremoses, multiprocess, docker-pycreds, sentry-sdk, gitdb, fairscale, seqeval, huggingface-hub, GitPython, wandb, transformers, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed GitPython-3.1.30 datasets-1.11.0 docker-pycreds-0.4.0 fairscale-0.4.0 gitdb-4.0.10 huggingface-hub-0.0.12 multiprocess-0.70.14 pathtools-0.1.2 sacremoses-0.0.53 sentry-sdk-1.13.0 seqeval-1.2.2 setproctitle-1.3.2 sklearn-0.0.post1 smmap-5.0.0 tokenizers-0.10.3 transformers-4.9.1 urllib3-1.26.14 wandb-0.13.9 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWQeMOCja5Ru",
        "outputId": "6b3d26ff-5d8a-4a9a-83c4-a41b9d9ce9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-13 04:19:25--  https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 458233246 (437M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 437.00M  17.1MB/s    in 30s     \n",
            "\n",
            "2023-01-13 04:19:56 (14.6 MB/s) - ‘data.zip’ saved [458233246/458233246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgROPlpmc8KQ",
        "outputId": "02252925-1231-449b-ce98-48c3916d6efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/tokcls/\n",
            "   creating: data/tokcls/BC5CDR-disease_hf/\n",
            "  inflating: data/tokcls/BC5CDR-disease_hf/test.json  \n",
            "  inflating: data/tokcls/BC5CDR-disease_hf/train.json  \n",
            "  inflating: data/tokcls/BC5CDR-disease_hf/dev.json  \n",
            "   creating: data/tokcls/BC2GM_hf/\n",
            "  inflating: data/tokcls/BC2GM_hf/train.json  \n",
            "  inflating: data/tokcls/BC2GM_hf/dev.json  \n",
            "  inflating: data/tokcls/BC2GM_hf/test.json  \n",
            "   creating: data/tokcls/ebmnlp_hf/\n",
            "  inflating: data/tokcls/ebmnlp_hf/test.json  \n",
            "  inflating: data/tokcls/ebmnlp_hf/train.json  \n",
            "  inflating: data/tokcls/ebmnlp_hf/dev.json  \n",
            "   creating: data/tokcls/NCBI-disease_hf/\n",
            "  inflating: data/tokcls/NCBI-disease_hf/test.json  \n",
            "  inflating: data/tokcls/NCBI-disease_hf/dev.json  \n",
            "  inflating: data/tokcls/NCBI-disease_hf/train.json  \n",
            "   creating: data/tokcls/JNLPBA_hf/\n",
            "  inflating: data/tokcls/JNLPBA_hf/dev.json  \n",
            "  inflating: data/tokcls/JNLPBA_hf/train.json  \n",
            "  inflating: data/tokcls/JNLPBA_hf/test.json  \n",
            "   creating: data/tokcls/BC5CDR-chem_hf/\n",
            "  inflating: data/tokcls/BC5CDR-chem_hf/dev.json  \n",
            "  inflating: data/tokcls/BC5CDR-chem_hf/test.json  \n",
            "  inflating: data/tokcls/BC5CDR-chem_hf/train.json  \n",
            "   creating: data/mc/\n",
            "   creating: data/mc/mmlu_hf/\n",
            "   creating: data/mc/mmlu_hf/professional_medicine/\n",
            "  inflating: data/mc/mmlu_hf/professional_medicine/dev.json  \n",
            "  inflating: data/mc/mmlu_hf/professional_medicine/test.json  \n",
            "  inflating: data/mc/mmlu_hf/professional_medicine/val.json  \n",
            "   creating: data/mc/medqa_usmle_hf/\n",
            "  inflating: data/mc/medqa_usmle_hf/test.json  \n",
            "  inflating: data/mc/medqa_usmle_hf/dev.json  \n",
            "  inflating: data/mc/medqa_usmle_hf/train.json  \n",
            "   creating: data/qa/\n",
            "   creating: data/qa/naturalqa_hf/\n",
            "  inflating: data/qa/naturalqa_hf/train_0.1.json  \n",
            "  inflating: data/qa/naturalqa_hf/train.json  \n",
            "  inflating: data/qa/naturalqa_hf/test.json  \n",
            "  inflating: data/qa/naturalqa_hf/dev.json  \n",
            "   creating: data/qa/triviaqa_hf/\n",
            "  inflating: data/qa/triviaqa_hf/train_0.1.json  \n",
            "  inflating: data/qa/triviaqa_hf/train.json  \n",
            "  inflating: data/qa/triviaqa_hf/dev.json  \n",
            "  inflating: data/qa/triviaqa_hf/test.json  \n",
            "   creating: data/qa/squad_hf/\n",
            "  inflating: data/qa/squad_hf/test.json  \n",
            "  inflating: data/qa/squad_hf/dev.json  \n",
            "  inflating: data/qa/squad_hf/train_0.1.json  \n",
            "  inflating: data/qa/squad_hf/train.json  \n",
            "   creating: data/qa/newsqa_hf/\n",
            "  inflating: data/qa/newsqa_hf/dev.json  \n",
            "  inflating: data/qa/newsqa_hf/test.json  \n",
            "  inflating: data/qa/newsqa_hf/train.json  \n",
            "   creating: data/qa/searchqa_hf/\n",
            "  inflating: data/qa/searchqa_hf/test.json  \n",
            "  inflating: data/qa/searchqa_hf/train.json  \n",
            "  inflating: data/qa/searchqa_hf/dev.json  \n",
            "   creating: data/qa/hotpot_hf/\n",
            "  inflating: data/qa/hotpot_hf/test.json  \n",
            "  inflating: data/qa/hotpot_hf/dev.json  \n",
            "  inflating: data/qa/hotpot_hf/train.json  \n",
            "  inflating: data/qa/hotpot_hf/train_0.1.json  \n",
            "   creating: data/seqcls/\n",
            "   creating: data/seqcls/hoc_hf/\n",
            "  inflating: data/seqcls/hoc_hf/dev.json  \n",
            "  inflating: data/seqcls/hoc_hf/train.json  \n",
            "  inflating: data/seqcls/hoc_hf/test.json  \n",
            "   creating: data/seqcls/BIOSSES_hf/\n",
            "  inflating: data/seqcls/BIOSSES_hf/train.json  \n",
            "  inflating: data/seqcls/BIOSSES_hf/dev.json  \n",
            "  inflating: data/seqcls/BIOSSES_hf/test.json  \n",
            "   creating: data/seqcls/GAD_hf/\n",
            "  inflating: data/seqcls/GAD_hf/train.json  \n",
            "  inflating: data/seqcls/GAD_hf/dev.json  \n",
            "  inflating: data/seqcls/GAD_hf/test.json  \n",
            "   creating: data/seqcls/HoC_hf/\n",
            "  inflating: data/seqcls/HoC_hf/dev.json  \n",
            "  inflating: data/seqcls/HoC_hf/test.json  \n",
            "  inflating: data/seqcls/HoC_hf/train.json  \n",
            "   creating: data/seqcls/chemprot_hf/\n",
            "  inflating: data/seqcls/chemprot_hf/train.json  \n",
            "  inflating: data/seqcls/chemprot_hf/dev.json  \n",
            "  inflating: data/seqcls/chemprot_hf/test.json  \n",
            "   creating: data/seqcls/bioasq_hf/\n",
            "  inflating: data/seqcls/bioasq_hf/test.json  \n",
            "  inflating: data/seqcls/bioasq_hf/dev.json  \n",
            "  inflating: data/seqcls/bioasq_hf/train.json  \n",
            "   creating: data/seqcls/pubmedqa_hf/\n",
            "  inflating: data/seqcls/pubmedqa_hf/dev.json  \n",
            "  inflating: data/seqcls/pubmedqa_hf/test.json  \n",
            "  inflating: data/seqcls/pubmedqa_hf/train.json  \n",
            "   creating: data/seqcls/DDI_hf/\n",
            "  inflating: data/seqcls/DDI_hf/train.json  \n",
            "  inflating: data/seqcls/DDI_hf/dev.json  \n",
            "  inflating: data/seqcls/DDI_hf/test.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb"
      ],
      "metadata": {
        "id": "sXw1RNBYeiq5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b86760-0e0b-476a-ae7e-219431880f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.8/dist-packages (0.13.9)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.8/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.8/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.1.30)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.13.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from GitPython>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s10z2Tir_z_g",
        "outputId": "d1c7f9cf-c3ed-4033-bd4a-57ac4c619fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd LinkBERT/src\n",
        "\n",
        "MODEL=BioLinkBERT-base\n",
        "MODEL_PATH=michiyasunaga/$MODEL\n",
        "\n",
        "task=NCBI-disease_hf\n",
        "datadir=../../data/tokcls/$task\n",
        "outdir=runs/$task/$MODEL\n",
        "mkdir -p $outdir\n",
        "python3 -u tokcls/run_ner.py --model_name_or_path $MODEL_PATH \\\n",
        "  --train_file $datadir/train.json --validation_file $datadir/dev.json --test_file $datadir/test.json \\\n",
        "  --do_train --do_eval --do_predict \\\n",
        "  --per_device_train_batch_size 32 --gradient_accumulation_steps 1 --fp16 \\\n",
        "  --learning_rate 5e-5 --warmup_ratio 0.1 --num_train_epochs 20 --max_seq_length 512 \\\n",
        "  --save_strategy no --evaluation_strategy no --output_dir $outdir --overwrite_output_dir \\\n",
        "  |& tee $outdir/log.txt &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TZ3yxzCWHSX",
        "outputId": "4393f3d9-991d-4654-cb1b-7744d6ed1bd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=runs/NCBI-disease_hf/BioLinkBERT-base/runs/Jan13_04-32-57_6d797a5a4c65,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=20.0,\n",
            "output_dir=runs/NCBI-disease_hf/BioLinkBERT-base,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=BioLinkBERT-base,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=runs/NCBI-disease_hf/BioLinkBERT-base,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.1,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-10982b564af71b9c\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-10982b564af71b9c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264)\n",
            "Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/json/default-10982b564af71b9c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264...\n",
            "100%|██████████| 3/3 [00:00<00:00, 9731.56it/s]\n",
            "INFO:datasets.utils.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.utils.download_manager:Checksum Computation took 0.0 min\n",
            "100%|██████████| 3/3 [00:00<00:00, 1412.86it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating split train\n",
            "INFO:datasets.builder:Generating split validation\n",
            "INFO:datasets.builder:Generating split test\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-10982b564af71b9c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264. Subsequent calls will reuse this data.\n",
            "100%|██████████| 3/3 [00:00<00:00, 61.83it/s]\n",
            "[INFO|file_utils.py:1624] 2023-01-13 04:32:59,821 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmplx12xtae\n",
            "Downloading: 100%|██████████| 559/559 [00:00<00:00, 406kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 04:33:00,728 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ad032c76cac1f75bba037ba006dcccc1c62ab157749b194df023bfa55e5f4fbf.22ae3f7c73ebda8488a8505a67c1b929a707ae7db67a129f60b7c28acfc38436\n",
            "[INFO|file_utils.py:1636] 2023-01-13 04:33:00,728 >> creating metadata file for /root/.cache/huggingface/transformers/ad032c76cac1f75bba037ba006dcccc1c62ab157749b194df023bfa55e5f4fbf.22ae3f7c73ebda8488a8505a67c1b929a707ae7db67a129f60b7c28acfc38436\n",
            "[INFO|configuration_utils.py:545] 2023-01-13 04:33:00,729 >> loading configuration file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad032c76cac1f75bba037ba006dcccc1c62ab157749b194df023bfa55e5f4fbf.22ae3f7c73ebda8488a8505a67c1b929a707ae7db67a129f60b7c28acfc38436\n",
            "[INFO|configuration_utils.py:581] 2023-01-13 04:33:00,729 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"ner\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"B\",\n",
            "    \"1\": \"I\",\n",
            "    \"2\": \"O\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"B\": 0,\n",
            "    \"I\": 1,\n",
            "    \"O\": 2\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1624] 2023-01-13 04:33:01,637 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4qp9p1bq\n",
            "Downloading: 100%|██████████| 379/379 [00:00<00:00, 244kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 04:33:02,547 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/30e2841862fd496cf36bc8647c9633a1dc319fbf6cc88a80438ca3f89e28339b.fab032bd2aab224bad4dcfc35e3bd6122976da1fa23e4feeb97d8fa65491aded\n",
            "[INFO|file_utils.py:1636] 2023-01-13 04:33:02,548 >> creating metadata file for /root/.cache/huggingface/transformers/30e2841862fd496cf36bc8647c9633a1dc319fbf6cc88a80438ca3f89e28339b.fab032bd2aab224bad4dcfc35e3bd6122976da1fa23e4feeb97d8fa65491aded\n",
            "[INFO|file_utils.py:1624] 2023-01-13 04:33:04,377 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxf3d6l_6\n",
            "Downloading: 100%|██████████| 225k/225k [00:00<00:00, 254kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 04:33:06,174 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/9eb712b5fcba51331b49cb69f18de1577371a2582055a298e2546c0c97d3b924.73b5c069d3e40205dd2df2379051c9f47d13c3bad0bcb3cee659c69e3a185a86\n",
            "[INFO|file_utils.py:1636] 2023-01-13 04:33:06,175 >> creating metadata file for /root/.cache/huggingface/transformers/9eb712b5fcba51331b49cb69f18de1577371a2582055a298e2546c0c97d3b924.73b5c069d3e40205dd2df2379051c9f47d13c3bad0bcb3cee659c69e3a185a86\n",
            "[INFO|file_utils.py:1624] 2023-01-13 04:33:07,080 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxqk6wl_7\n",
            "Downloading: 100%|██████████| 447k/447k [00:01<00:00, 401kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 04:33:09,118 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/3c720cf86b025f815b1d833b6b39db05e8e7493b6f6a87788c485a946848b4d8.a25e24b89fd9bfd32e3c8d2dbb39879c62152e7f069ab24c97198c004cad94c9\n",
            "[INFO|file_utils.py:1636] 2023-01-13 04:33:09,118 >> creating metadata file for /root/.cache/huggingface/transformers/3c720cf86b025f815b1d833b6b39db05e8e7493b6f6a87788c485a946848b4d8.a25e24b89fd9bfd32e3c8d2dbb39879c62152e7f069ab24c97198c004cad94c9\n",
            "[INFO|file_utils.py:1624] 2023-01-13 04:33:10,930 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp9daot9dm\n",
            "Downloading: 100%|██████████| 112/112 [00:00<00:00, 79.0kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 04:33:11,840 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/0598867425495ec6baf3617ab3789f3d8b84ebf869f7b43aa4a2930195a74dbe.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|file_utils.py:1636] 2023-01-13 04:33:11,840 >> creating metadata file for /root/.cache/huggingface/transformers/0598867425495ec6baf3617ab3789f3d8b84ebf869f7b43aa4a2930195a74dbe.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 04:33:12,751 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/9eb712b5fcba51331b49cb69f18de1577371a2582055a298e2546c0c97d3b924.73b5c069d3e40205dd2df2379051c9f47d13c3bad0bcb3cee659c69e3a185a86\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 04:33:12,751 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/3c720cf86b025f815b1d833b6b39db05e8e7493b6f6a87788c485a946848b4d8.a25e24b89fd9bfd32e3c8d2dbb39879c62152e7f069ab24c97198c004cad94c9\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 04:33:12,751 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 04:33:12,751 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/0598867425495ec6baf3617ab3789f3d8b84ebf869f7b43aa4a2930195a74dbe.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 04:33:12,751 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/30e2841862fd496cf36bc8647c9633a1dc319fbf6cc88a80438ca3f89e28339b.fab032bd2aab224bad4dcfc35e3bd6122976da1fa23e4feeb97d8fa65491aded\n",
            "[INFO|file_utils.py:1624] 2023-01-13 04:33:13,683 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpej40aw6y\n",
            "Downloading: 100%|██████████| 433M/433M [00:25<00:00, 17.3MB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 04:33:39,772 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/76a88449a3eb7019bbc0d164cc39a6a231c8bbe3b9678b8d40977424f0ad934d.f8b95ad9e1dea734685fba5a5b6142b539678b7fc2311981cc14ae61b19f709d\n",
            "[INFO|file_utils.py:1636] 2023-01-13 04:33:39,773 >> creating metadata file for /root/.cache/huggingface/transformers/76a88449a3eb7019bbc0d164cc39a6a231c8bbe3b9678b8d40977424f0ad934d.f8b95ad9e1dea734685fba5a5b6142b539678b7fc2311981cc14ae61b19f709d\n",
            "[INFO|modeling_utils.py:1271] 2023-01-13 04:33:39,773 >> loading weights file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/76a88449a3eb7019bbc0d164cc39a6a231c8bbe3b9678b8d40977424f0ad934d.f8b95ad9e1dea734685fba5a5b6142b539678b7fc2311981cc14ae61b19f709d\n",
            "[INFO|modeling_utils.py:1510] 2023-01-13 04:33:41,097 >> All model checkpoint weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:1512] 2023-01-13 04:33:41,097 >> Some weights of BertForTokenClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7f81b9480dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on train dataset:   0%|          | 0/6 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-10982b564af71b9c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on train dataset: 100%|██████████| 6/6 [00:01<00:00,  4.48ba/s]\n",
            "INFO:datasets.fingerprint:Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7f81b9480dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on validation dataset:   0%|          | 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-10982b564af71b9c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on validation dataset: 100%|██████████| 1/1 [00:00<00:00,  5.43ba/s]\n",
            "INFO:datasets.fingerprint:Parameter 'function'=<function main.<locals>.tokenize_and_align_labels at 0x7f81b9480dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on prediction dataset:   0%|          | 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-10982b564af71b9c/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-3eb13b9046685257.arrow\n",
            "Running tokenizer on prediction dataset: 100%|██████████| 1/1 [00:00<00:00,  5.38ba/s]\n",
            "INFO:datasets.utils.file_utils:https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/seqeval/seqeval.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpfudycfmg\n",
            "Downloading: 6.34kB [00:00, 5.90MB/s]                   \n",
            "INFO:datasets.utils.file_utils:storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/seqeval/seqeval.py in cache at /root/.cache/huggingface/datasets/downloads/131e4c55b818ffd7dd4d98d2cf9b9255020a0c2381f20b72ea2c38660ec57209.a09051c6235c8b054e473ffcc08da34f95b4a1800e1818bce66eed11f4e3833d.py\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/131e4c55b818ffd7dd4d98d2cf9b9255020a0c2381f20b72ea2c38660ec57209.a09051c6235c8b054e473ffcc08da34f95b4a1800e1818bce66eed11f4e3833d.py\n",
            "INFO:datasets.load:Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval\n",
            "INFO:datasets.load:Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e\n",
            "INFO:datasets.load:Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/seqeval/seqeval.py to /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.py\n",
            "INFO:datasets.load:Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/seqeval/dataset_infos.json\n",
            "INFO:datasets.load:Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/seqeval/seqeval.py at /root/.cache/huggingface/modules/datasets_modules/metrics/seqeval/1fde2544ac1f3f7e54c639c73221d3a5e5377d2213b9b0fdb579b96980b84b2e/seqeval.json\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:421] 2023-01-13 04:33:55,075 >> Using amp fp16 backend\n",
            "[INFO|trainer.py:521] 2023-01-13 04:33:55,075 >> The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens, word_ids.\n",
            "[INFO|trainer.py:1164] 2023-01-13 04:33:55,089 >> ***** Running training *****\n",
            "[INFO|trainer.py:1165] 2023-01-13 04:33:55,089 >>   Num examples = 5424\n",
            "[INFO|trainer.py:1166] 2023-01-13 04:33:55,089 >>   Num Epochs = 20\n",
            "[INFO|trainer.py:1167] 2023-01-13 04:33:55,089 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1168] 2023-01-13 04:33:55,089 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1169] 2023-01-13 04:33:55,089 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1170] 2023-01-13 04:33:55,089 >>   Total optimization steps = 3400\n",
            "[INFO|integrations.py:445] 2023-01-13 04:33:55,103 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "wandb: Currently logged in as: derekliu2021. Use `wandb login --relogin` to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "wandb: Tracking run with wandb version 0.13.9\n",
            "wandb: Run data is saved locally in /content/LinkBERT/src/wandb/run-20230113_043357-28s5023x\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run runs/NCBI-disease_hf/BioLinkBERT-base\n",
            "wandb: ⭐️ View project at https://wandb.ai/derekliu2021/huggingface\n",
            "wandb: 🚀 View run at https://wandb.ai/derekliu2021/huggingface/runs/28s5023x\n",
            "{'loss': 0.1521, 'learning_rate': 4.7434640522875814e-05, 'epoch': 2.94}\n",
            "{'loss': 0.0109, 'learning_rate': 3.9264705882352945e-05, 'epoch': 5.88}\n",
            "{'loss': 0.0037, 'learning_rate': 3.109477124183007e-05, 'epoch': 8.82}\n",
            "{'loss': 0.0016, 'learning_rate': 2.292483660130719e-05, 'epoch': 11.76}\n",
            "{'loss': 0.0008, 'learning_rate': 1.4754901960784315e-05, 'epoch': 14.71}\n",
            "{'loss': 0.0005, 'learning_rate': 6.584967320261438e-06, 'epoch': 17.65}\n",
            "100%|█████████▉| 3399/3400 [08:51<00:00,  6.52it/s][INFO|trainer.py:1360] 2023-01-13 04:42:49,832 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 534.7432, 'train_samples_per_second': 202.864, 'train_steps_per_second': 6.358, 'train_loss': 0.02497022503439118, 'epoch': 20.0}\n",
            "100%|██████████| 3400/3400 [08:51<00:00,  6.40it/s]\n",
            "[INFO|trainer.py:1919] 2023-01-13 04:42:49,838 >> Saving model checkpoint to runs/NCBI-disease_hf/BioLinkBERT-base\n",
            "[INFO|configuration_utils.py:379] 2023-01-13 04:42:49,839 >> Configuration saved in runs/NCBI-disease_hf/BioLinkBERT-base/config.json\n",
            "[INFO|modeling_utils.py:997] 2023-01-13 04:42:51,144 >> Model weights saved in runs/NCBI-disease_hf/BioLinkBERT-base/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2023-01-13 04:42:51,147 >> tokenizer config file saved in runs/NCBI-disease_hf/BioLinkBERT-base/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2023-01-13 04:42:51,148 >> Special tokens file saved in runs/NCBI-disease_hf/BioLinkBERT-base/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       20.0\n",
            "  train_loss               =      0.025\n",
            "  train_runtime            = 0:08:54.74\n",
            "  train_samples            =       5424\n",
            "  train_samples_per_second =    202.864\n",
            "  train_steps_per_second   =      6.358\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:521] 2023-01-13 04:42:51,531 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens, word_ids.\n",
            "[INFO|trainer.py:2165] 2023-01-13 04:42:51,535 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2023-01-13 04:42:51,535 >>   Num examples = 923\n",
            "[INFO|trainer.py:2170] 2023-01-13 04:42:51,536 >>   Batch size = 8\n",
            " 97%|█████████▋| 113/116 [00:03<00:00, 35.64it/s]INFO:datasets.metric:Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "100%|██████████| 116/116 [00:03<00:00, 31.87it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       20.0\n",
            "  eval_accuracy           =     0.9869\n",
            "  eval_f1                 =     0.8688\n",
            "  eval_loss               =     0.0888\n",
            "  eval_precision          =     0.8586\n",
            "  eval_recall             =     0.8793\n",
            "  eval_runtime            = 0:00:03.71\n",
            "  eval_samples            =        923\n",
            "  eval_samples_per_second =    248.417\n",
            "  eval_steps_per_second   =      31.22\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:521] 2023-01-13 04:42:55,268 >> The following columns in the test set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: id, ner_tags, tokens, word_ids.\n",
            "[INFO|trainer.py:2165] 2023-01-13 04:42:55,272 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2167] 2023-01-13 04:42:55,272 >>   Num examples = 940\n",
            "[INFO|trainer.py:2170] 2023-01-13 04:42:55,273 >>   Batch size = 8\n",
            " 97%|█████████▋| 114/118 [00:03<00:00, 34.30it/s]INFO:datasets.metric:Removing /root/.cache/huggingface/metrics/seqeval/default/default_experiment-1-0.arrow\n",
            "***** test metrics *****\n",
            "  test_accuracy           =     0.9862\n",
            "  test_f1                 =     0.8789\n",
            "  test_loss               =     0.0941\n",
            "  test_precision          =     0.8664\n",
            "  test_recall             =     0.8917\n",
            "  test_runtime            = 0:00:03.73\n",
            "  test_samples            =        940\n",
            "  test_samples_per_second =    251.599\n",
            "  test_steps_per_second   =     31.584\n",
            "100%|██████████| 118/118 [00:04<00:00, 25.93it/s]\n",
            "wandb: Waiting for W&B process to finish... (success).\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:                  eval/accuracy ▁\n",
            "wandb:                        eval/f1 ▁\n",
            "wandb:                      eval/loss ▁\n",
            "wandb:                 eval/precision ▁\n",
            "wandb:                    eval/recall ▁\n",
            "wandb:                   eval/runtime ▁\n",
            "wandb:        eval/samples_per_second ▁\n",
            "wandb:          eval/steps_per_second ▁\n",
            "wandb:                    train/epoch ▁▂▃▅▆▇███\n",
            "wandb:              train/global_step ▁▂▃▅▆▇███\n",
            "wandb:            train/learning_rate █▇▅▄▂▁\n",
            "wandb:                     train/loss █▁▁▁▁▁\n",
            "wandb:            train/test_accuracy ▁\n",
            "wandb:                  train/test_f1 ▁\n",
            "wandb:                train/test_loss ▁\n",
            "wandb:           train/test_precision ▁\n",
            "wandb:              train/test_recall ▁\n",
            "wandb:             train/test_runtime ▁\n",
            "wandb:             train/test_samples ▁\n",
            "wandb:  train/test_samples_per_second ▁\n",
            "wandb:    train/test_steps_per_second ▁\n",
            "wandb:               train/total_flos ▁\n",
            "wandb:               train/train_loss ▁\n",
            "wandb:            train/train_runtime ▁\n",
            "wandb: train/train_samples_per_second ▁\n",
            "wandb:   train/train_steps_per_second ▁\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:                  eval/accuracy 0.98694\n",
            "wandb:                        eval/f1 0.8688\n",
            "wandb:                      eval/loss 0.08881\n",
            "wandb:                 eval/precision 0.85856\n",
            "wandb:                    eval/recall 0.87929\n",
            "wandb:                   eval/runtime 3.7155\n",
            "wandb:        eval/samples_per_second 248.417\n",
            "wandb:          eval/steps_per_second 31.22\n",
            "wandb:                    train/epoch 20.0\n",
            "wandb:              train/global_step 3400\n",
            "wandb:            train/learning_rate 1e-05\n",
            "wandb:                     train/loss 0.0005\n",
            "wandb:            train/test_accuracy 0.98624\n",
            "wandb:                  train/test_f1 0.87885\n",
            "wandb:                train/test_loss 0.09415\n",
            "wandb:           train/test_precision 0.8664\n",
            "wandb:              train/test_recall 0.89167\n",
            "wandb:             train/test_runtime 3.7361\n",
            "wandb:             train/test_samples 940\n",
            "wandb:  train/test_samples_per_second 251.599\n",
            "wandb:    train/test_steps_per_second 31.584\n",
            "wandb:               train/total_flos 3893552848988928.0\n",
            "wandb:               train/train_loss 0.02497\n",
            "wandb:            train/train_runtime 534.7432\n",
            "wandb: train/train_samples_per_second 202.864\n",
            "wandb:   train/train_steps_per_second 6.358\n",
            "wandb: \n",
            "wandb: 🚀 View run runs/NCBI-disease_hf/BioLinkBERT-base at: https://wandb.ai/derekliu2021/huggingface/runs/28s5023x\n",
            "wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "wandb: Find logs at: ./wandb/run-20230113_043357-28s5023x/logs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "cd LinkBERT/src\n",
        "\n",
        "MODEL=BioLinkBERT-base\n",
        "MODEL_PATH=michiyasunaga/$MODEL\n",
        "\n",
        "task=GAD_hf\n",
        "datadir=../../data/seqcls/$task\n",
        "outdir=runs/$task/$MODEL\n",
        "mkdir -p $outdir\n",
        "python3 -u seqcls/run_seqcls.py --model_name_or_path $MODEL_PATH \\\n",
        "  --train_file $datadir/train.json --validation_file $datadir/dev.json --test_file $datadir/test.json \\\n",
        "  --do_train --do_eval --do_predict --metric_name PRF1 \\\n",
        "  --per_device_train_batch_size 32 --gradient_accumulation_steps 1 --fp16 \\\n",
        "  --learning_rate 3e-5 --num_train_epochs 10 --max_seq_length 256 \\\n",
        "  --save_strategy no --evaluation_strategy no --output_dir $outdir --overwrite_output_dir \\\n",
        "  |& tee $outdir/log.txt &"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1jLYxwUd0zj",
        "outputId": "78d60242-755d-41e8-83a6-90c247a272f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "INFO:__main__:Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=runs/GAD_hf/BioLinkBERT-base/runs/Jan13_03-07-33_ddf17b5aa518,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "output_dir=runs/GAD_hf/BioLinkBERT-base,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=BioLinkBERT-base,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=runs/GAD_hf/BioLinkBERT-base,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.NO,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "INFO:__main__:load a local file for train: ../../data/seqcls/GAD_hf/train.json\n",
            "INFO:__main__:load a local file for validation: ../../data/seqcls/GAD_hf/dev.json\n",
            "INFO:__main__:load a local file for test: ../../data/seqcls/GAD_hf/test.json\n",
            "WARNING:datasets.builder:Using custom data configuration default-8ff6fbb89ac24da2\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8ff6fbb89ac24da2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264\n",
            "WARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-8ff6fbb89ac24da2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-8ff6fbb89ac24da2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264\n",
            "100%|██████████| 3/3 [00:00<00:00, 316.63it/s]\n",
            "\n",
            "label_list ['0', '1']\n",
            "[INFO|file_utils.py:1624] 2023-01-13 03:07:34,633 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5ieibo96\n",
            "Downloading: 100%|██████████| 559/559 [00:00<00:00, 375kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 03:07:35,007 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/ad032c76cac1f75bba037ba006dcccc1c62ab157749b194df023bfa55e5f4fbf.22ae3f7c73ebda8488a8505a67c1b929a707ae7db67a129f60b7c28acfc38436\n",
            "[INFO|file_utils.py:1636] 2023-01-13 03:07:35,008 >> creating metadata file for /root/.cache/huggingface/transformers/ad032c76cac1f75bba037ba006dcccc1c62ab157749b194df023bfa55e5f4fbf.22ae3f7c73ebda8488a8505a67c1b929a707ae7db67a129f60b7c28acfc38436\n",
            "[INFO|configuration_utils.py:545] 2023-01-13 03:07:35,008 >> loading configuration file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/ad032c76cac1f75bba037ba006dcccc1c62ab157749b194df023bfa55e5f4fbf.22ae3f7c73ebda8488a8505a67c1b929a707ae7db67a129f60b7c28acfc38436\n",
            "[INFO|configuration_utils.py:581] 2023-01-13 03:07:35,009 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.9.1\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28895\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1624] 2023-01-13 03:07:35,369 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjiylt6cp\n",
            "Downloading: 100%|██████████| 379/379 [00:00<00:00, 272kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 03:07:35,731 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/30e2841862fd496cf36bc8647c9633a1dc319fbf6cc88a80438ca3f89e28339b.fab032bd2aab224bad4dcfc35e3bd6122976da1fa23e4feeb97d8fa65491aded\n",
            "[INFO|file_utils.py:1636] 2023-01-13 03:07:35,731 >> creating metadata file for /root/.cache/huggingface/transformers/30e2841862fd496cf36bc8647c9633a1dc319fbf6cc88a80438ca3f89e28339b.fab032bd2aab224bad4dcfc35e3bd6122976da1fa23e4feeb97d8fa65491aded\n",
            "[INFO|file_utils.py:1624] 2023-01-13 03:07:36,499 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvx8b_z1e\n",
            "Downloading: 100%|██████████| 225k/225k [00:00<00:00, 657kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 03:07:37,207 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/9eb712b5fcba51331b49cb69f18de1577371a2582055a298e2546c0c97d3b924.73b5c069d3e40205dd2df2379051c9f47d13c3bad0bcb3cee659c69e3a185a86\n",
            "[INFO|file_utils.py:1636] 2023-01-13 03:07:37,207 >> creating metadata file for /root/.cache/huggingface/transformers/9eb712b5fcba51331b49cb69f18de1577371a2582055a298e2546c0c97d3b924.73b5c069d3e40205dd2df2379051c9f47d13c3bad0bcb3cee659c69e3a185a86\n",
            "[INFO|file_utils.py:1624] 2023-01-13 03:07:37,566 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpbf7csvem\n",
            "Downloading: 100%|██████████| 447k/447k [00:00<00:00, 1.04MB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 03:07:38,365 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/3c720cf86b025f815b1d833b6b39db05e8e7493b6f6a87788c485a946848b4d8.a25e24b89fd9bfd32e3c8d2dbb39879c62152e7f069ab24c97198c004cad94c9\n",
            "[INFO|file_utils.py:1636] 2023-01-13 03:07:38,365 >> creating metadata file for /root/.cache/huggingface/transformers/3c720cf86b025f815b1d833b6b39db05e8e7493b6f6a87788c485a946848b4d8.a25e24b89fd9bfd32e3c8d2dbb39879c62152e7f069ab24c97198c004cad94c9\n",
            "[INFO|file_utils.py:1624] 2023-01-13 03:07:39,085 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpo64o0aqy\n",
            "Downloading: 100%|██████████| 112/112 [00:00<00:00, 79.6kB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 03:07:39,444 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/0598867425495ec6baf3617ab3789f3d8b84ebf869f7b43aa4a2930195a74dbe.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|file_utils.py:1636] 2023-01-13 03:07:39,445 >> creating metadata file for /root/.cache/huggingface/transformers/0598867425495ec6baf3617ab3789f3d8b84ebf869f7b43aa4a2930195a74dbe.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 03:07:39,805 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/9eb712b5fcba51331b49cb69f18de1577371a2582055a298e2546c0c97d3b924.73b5c069d3e40205dd2df2379051c9f47d13c3bad0bcb3cee659c69e3a185a86\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 03:07:39,805 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/3c720cf86b025f815b1d833b6b39db05e8e7493b6f6a87788c485a946848b4d8.a25e24b89fd9bfd32e3c8d2dbb39879c62152e7f069ab24c97198c004cad94c9\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 03:07:39,805 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 03:07:39,805 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/0598867425495ec6baf3617ab3789f3d8b84ebf869f7b43aa4a2930195a74dbe.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
            "[INFO|tokenization_utils_base.py:1730] 2023-01-13 03:07:39,805 >> loading file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/30e2841862fd496cf36bc8647c9633a1dc319fbf6cc88a80438ca3f89e28339b.fab032bd2aab224bad4dcfc35e3bd6122976da1fa23e4feeb97d8fa65491aded\n",
            "[INFO|file_utils.py:1624] 2023-01-13 03:07:40,187 >> https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpr65il_51\n",
            "Downloading: 100%|██████████| 433M/433M [00:04<00:00, 93.5MB/s]\n",
            "[INFO|file_utils.py:1628] 2023-01-13 03:07:44,889 >> storing https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/76a88449a3eb7019bbc0d164cc39a6a231c8bbe3b9678b8d40977424f0ad934d.f8b95ad9e1dea734685fba5a5b6142b539678b7fc2311981cc14ae61b19f709d\n",
            "[INFO|file_utils.py:1636] 2023-01-13 03:07:44,889 >> creating metadata file for /root/.cache/huggingface/transformers/76a88449a3eb7019bbc0d164cc39a6a231c8bbe3b9678b8d40977424f0ad934d.f8b95ad9e1dea734685fba5a5b6142b539678b7fc2311981cc14ae61b19f709d\n",
            "[INFO|modeling_utils.py:1271] 2023-01-13 03:07:44,890 >> loading weights file https://huggingface.co/michiyasunaga/BioLinkBERT-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/76a88449a3eb7019bbc0d164cc39a6a231c8bbe3b9678b8d40977424f0ad934d.f8b95ad9e1dea734685fba5a5b6142b539678b7fc2311981cc14ae61b19f709d\n",
            "[INFO|modeling_utils.py:1510] 2023-01-13 03:07:46,094 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:1512] 2023-01-13 03:07:46,094 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at michiyasunaga/BioLinkBERT-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "WARNING:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fb1d64060d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8ff6fbb89ac24da2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-1c80317fa3b1799d.arrow\n",
            "INFO:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fb1d64060d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8ff6fbb89ac24da2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-bdd640fb06671ad1.arrow\n",
            "INFO:datasets.fingerprint:Parameter 'function'=<function main.<locals>.preprocess_function at 0x7fb1d64060d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-8ff6fbb89ac24da2/0.0.0/45636811569ec4a6630521c18235dfbbab83b7ab572e3393c5ba68ccabe98264/cache-3eb13b9046685257.arrow\n",
            "[INFO|trainer.py:421] 2023-01-13 03:07:52,378 >> Using amp fp16 backend\n",
            "[INFO|trainer.py:521] 2023-01-13 03:07:52,379 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, sentence.\n",
            "[INFO|trainer.py:1164] 2023-01-13 03:07:52,389 >> ***** Running training *****\n",
            "[INFO|trainer.py:1165] 2023-01-13 03:07:52,393 >>   Num examples = 4261\n",
            "[INFO|trainer.py:1166] 2023-01-13 03:07:52,394 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1167] 2023-01-13 03:07:52,394 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:1168] 2023-01-13 03:07:52,395 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:1169] 2023-01-13 03:07:52,395 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1170] 2023-01-13 03:07:52,396 >>   Total optimization steps = 1340\n",
            "[INFO|integrations.py:445] 2023-01-13 03:07:52,411 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "wandb: Currently logged in as: derekliu2021. Use `wandb login --relogin` to force relogin\n",
            "wandb: Tracking run with wandb version 0.13.9\n",
            "wandb: Run data is saved locally in /content/LinkBERT/src/wandb/run-20230113_030753-b2nxkrlj\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run runs/GAD_hf/BioLinkBERT-base\n",
            "wandb: ⭐️ View project at https://wandb.ai/derekliu2021/huggingface\n",
            "wandb: 🚀 View run at https://wandb.ai/derekliu2021/huggingface/runs/b2nxkrlj\n",
            "{'loss': 0.4076, 'learning_rate': 1.8828358208955224e-05, 'epoch': 3.73}\n",
            "{'loss': 0.1407, 'learning_rate': 7.634328358208955e-06, 'epoch': 7.46}\n",
            "100%|██████████| 1340/1340 [10:43<00:00,  2.63it/s][INFO|trainer.py:1360] 2023-01-13 03:18:38,259 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 645.8631, 'train_samples_per_second': 65.974, 'train_steps_per_second': 2.075, 'train_loss': 0.21968355890530258, 'epoch': 10.0}\n",
            "100%|██████████| 1340/1340 [10:43<00:00,  2.08it/s]\n",
            "[INFO|trainer.py:1919] 2023-01-13 03:18:38,265 >> Saving model checkpoint to runs/GAD_hf/BioLinkBERT-base\n",
            "[INFO|configuration_utils.py:379] 2023-01-13 03:18:38,267 >> Configuration saved in runs/GAD_hf/BioLinkBERT-base/config.json\n",
            "[INFO|modeling_utils.py:997] 2023-01-13 03:18:39,572 >> Model weights saved in runs/GAD_hf/BioLinkBERT-base/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2006] 2023-01-13 03:18:39,573 >> tokenizer config file saved in runs/GAD_hf/BioLinkBERT-base/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2012] 2023-01-13 03:18:39,574 >> Special tokens file saved in runs/GAD_hf/BioLinkBERT-base/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     0.2197\n",
            "  train_runtime            = 0:10:45.86\n",
            "  train_samples            =       4261\n",
            "  train_samples_per_second =     65.974\n",
            "  train_steps_per_second   =      2.075\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:521] 2023-01-13 03:18:39,699 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, sentence.\n",
            "[INFO|trainer.py:2165] 2023-01-13 03:18:39,703 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2167] 2023-01-13 03:18:39,703 >>   Num examples = 535\n",
            "[INFO|trainer.py:2170] 2023-01-13 03:18:39,703 >>   Batch size = 8\n",
            "100%|██████████| 67/67 [00:08<00:00,  8.11it/s]INFO:datasets.utils.file_utils:https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp989ah6pq\n",
            "\n",
            "Downloading: 3.21kB [00:00, 2.78MB/s]                   \n",
            "INFO:datasets.utils.file_utils:storing https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py in cache at /root/.cache/huggingface/datasets/downloads/591d385c0492ec80bad3323a081d7b1126c681a389acd9759f0a3098d2034ceb.1d0bd855d8e3c9eb14daa00edb910a511da1d653aeca8e7bc5dffb23a84d7b54.py\n",
            "INFO:datasets.utils.file_utils:creating metadata file for /root/.cache/huggingface/datasets/downloads/591d385c0492ec80bad3323a081d7b1126c681a389acd9759f0a3098d2034ceb.1d0bd855d8e3c9eb14daa00edb910a511da1d653aeca8e7bc5dffb23a84d7b54.py\n",
            "INFO:datasets.load:Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy\n",
            "INFO:datasets.load:Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b\n",
            "INFO:datasets.load:Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py to /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.py\n",
            "INFO:datasets.load:Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/dataset_infos.json\n",
            "INFO:datasets.load:Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.json\n",
            "100%|██████████| 67/67 [00:09<00:00,  7.27it/s]\n",
            "***** eval metrics *****\n",
            "  epoch          =   10.0\n",
            "  eval_F1        = 0.8463\n",
            "  eval_precision = 0.7899\n",
            "  eval_recall    = 0.9113\n",
            "  eval_samples   =    535\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:521] 2023-01-13 03:18:49,053 >> The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, sentence.\n",
            "[INFO|trainer.py:2165] 2023-01-13 03:18:49,058 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2167] 2023-01-13 03:18:49,058 >>   Num examples = 534\n",
            "[INFO|trainer.py:2170] 2023-01-13 03:18:49,059 >>   Batch size = 8\n",
            " 99%|█████████▊| 66/67 [00:08<00:00,  7.90it/s]INFO:datasets.load:Found main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy\n",
            "INFO:datasets.load:Found specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b\n",
            "INFO:datasets.load:Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py to /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.py\n",
            "INFO:datasets.load:Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/dataset_infos.json\n",
            "INFO:datasets.load:Found metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.11.0/metrics/accuracy/accuracy.py at /root/.cache/huggingface/modules/datasets_modules/metrics/accuracy/6dba4616f6b2bbd19659d1db3a48cc001c8f13a10cdc73a2641a55f7a60b7b5b/accuracy.json\n",
            "***** test metrics *****\n",
            "  epoch          =   10.0\n",
            "  test_F1        = 0.8159\n",
            "  test_precision =  0.764\n",
            "  test_recall    = 0.8754\n",
            "  test_samples   =    534\n",
            "100%|██████████| 67/67 [00:08<00:00,  7.64it/s]\n",
            "wandb: Waiting for W&B process to finish... (success).\n",
            "wandb: \n",
            "wandb: Run history:\n",
            "wandb:                        eval/F1 ▁\n",
            "wandb:                 eval/precision ▁\n",
            "wandb:                    eval/recall ▁\n",
            "wandb:                    train/epoch ▁▅████\n",
            "wandb:              train/global_step ▁▅████\n",
            "wandb:            train/learning_rate █▁\n",
            "wandb:                     train/loss █▁\n",
            "wandb:                  train/test_F1 ▁▁\n",
            "wandb:           train/test_precision ▁▁\n",
            "wandb:              train/test_recall ▁▁\n",
            "wandb:             train/test_samples ▁\n",
            "wandb:               train/total_flos ▁\n",
            "wandb:               train/train_loss ▁\n",
            "wandb:            train/train_runtime ▁\n",
            "wandb: train/train_samples_per_second ▁\n",
            "wandb:   train/train_steps_per_second ▁\n",
            "wandb: \n",
            "wandb: Run summary:\n",
            "wandb:                        eval/F1 0.84628\n",
            "wandb:                 eval/precision 0.78994\n",
            "wandb:                    eval/recall 0.91126\n",
            "wandb:                    train/epoch 10.0\n",
            "wandb:              train/global_step 1340\n",
            "wandb:            train/learning_rate 1e-05\n",
            "wandb:                     train/loss 0.1407\n",
            "wandb:                  train/test_F1 0.81592\n",
            "wandb:           train/test_precision 0.76398\n",
            "wandb:              train/test_recall 0.87544\n",
            "wandb:             train/test_samples 534\n",
            "wandb:               train/total_flos 5605581034444800.0\n",
            "wandb:               train/train_loss 0.21968\n",
            "wandb:            train/train_runtime 645.8631\n",
            "wandb: train/train_samples_per_second 65.974\n",
            "wandb:   train/train_steps_per_second 2.075\n",
            "wandb: \n",
            "wandb: 🚀 View run runs/GAD_hf/BioLinkBERT-base at: https://wandb.ai/derekliu2021/huggingface/runs/b2nxkrlj\n",
            "wandb: Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "wandb: Find logs at: ./wandb/run-20230113_030753-b2nxkrlj/logs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lkBsZh9ev6oW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}